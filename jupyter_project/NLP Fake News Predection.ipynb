{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification problem with Fake and Real news\n",
    "## BuffML www.buffml.com\n",
    "\n",
    "In this project, I build a text classification to define whether or not a certain article is fake news or real news. Using Natural Language Processing methodologies in Python and Classification Theory, I reached an accuracy of 0.945455 for classifying news as fake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## This file has all imports and helper functions used throughout the notebook\n",
    "%run python_helper.py\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean & Save Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspecting the data files, we noticed several issues for processing the traning dataset correctly. Using Regular Expression, we convert all commas between quotations to a pipe, so the CSV parsing works correctly with all values in their correct columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_str = open(\"fake_or_real_news_training.csv\", encoding= 'utf-8')\n",
    "\n",
    "# Remove all new lines\n",
    "noNewLines = re.sub(\"\\n\", \"\", input_str.read())\n",
    "  \n",
    "# re-add new line at end of each row\n",
    "noNewLines = re.sub(\"X1,X2\", \"X1,X2\\n\", noNewLines)\n",
    "  \n",
    "\n",
    "noNewLines = re.sub(\",FAKE[,]+\", \",FAKE,,\\n\", noNewLines)\n",
    "# noNewLines = re.sub(\",FAKE,(?!,)\",\",FAKE,,\\n\",noNewLines)\n",
    "# noNewLines = re.sub(\",FAKE,,(?!,)\",\",FAKE,,\\n\",noNewLines)\n",
    "  \n",
    "noNewLines = re.sub(\",REAL[,]+\", \",REAL,,\\n\", noNewLines)\n",
    "# noNewLines = re.sub(\",REAL,(?!,)\",\",REAL,,\\n\",noNewLines)\n",
    "# noNewLines = re.sub(\",REAL,,(?!,)\",\",REAL,,\\n\",noNewLines)\n",
    "  \n",
    "\n",
    "# Replace any commas between two quotes with |\n",
    "lines = noNewLines.split('\\n')\n",
    "\n",
    "def removeComma(g):\n",
    "      t = g.groups()\n",
    "      t = [t[0], t[1].replace(',', ' |'), t[2], t[3]]\n",
    "      return \"\".join(t)\n",
    "\n",
    "betweenQuotes = lambda line: re.sub(r'(.*,\")(.*)(\",)(.*)', lambda x: removeComma(x), line)\n",
    "\n",
    "secondCol = lambda line: re.sub(r'^([0-9]+,)(.*,.*)(,\\\")(.*)$', lambda x: removeComma(x), line, 1)\n",
    "\n",
    "\n",
    "lines = [betweenQuotes(l) for l in lines]\n",
    "lines = [secondCol(l) for l in lines]\n",
    "\n",
    "finalString = '\\n'.join(lines)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save cleaned file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('fake_or_real_news_training_CLEANED.csv', 'w',encoding= 'utf-8')\n",
    "file.write(finalString)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"fake_or_real_news_training_CLEANED.csv\")\n",
    "test = pd.read_csv(\"fake_or_real_news_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.drop(['X1', 'X2'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We study if the dataset is unbalanced. From the plot we see this is not the case, as there is a similar amount of Fake and Real news articles. No further actions have to be taken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "ax = sns.countplot(train.label, order=[x for x, count in sorted(Counter(train.label).items(), key=lambda x: -x[1])])\n",
    "\n",
    "\n",
    "for p in ax.patches:\n",
    "    height = p.get_height()\n",
    "    ax.text(p.get_x()+p.get_width()/2.,\n",
    "            height + 3,\n",
    "            '{:1.2f}%'.format(height/len(train)*100),\n",
    "            ha=\"center\") \n",
    "ax.set_title(\"Test dataset target\")\n",
    "show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to not do double work by doing operations on our train and testset and to analyze general distributions of our data, we stack train and test in df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['label'] = None  # empty label for test\n",
    "\n",
    "df = pd.concat([train, test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part we will be cleaning the articles with the help of different NLP techniques, of which we will first explain the concept and its importance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to take into account the title in our accuracy prediction, we created an extra column that combines text and title. We will not do separate predictions on the title since these might classify as e.g. Fake news, whether the actual text with more explanation tells a Real story."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['title_and_text'] = df['title'] +' '+ df['text']\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "preprocess() can be found in python_helper.py\n",
    "Here you can read the explanations of the preprocess steps we took\n",
    "\n",
    "1. lowercase the text\n",
    "\n",
    "This preprocessing step is done so words van later be cross checked with the stopword and pos_tag dictionaries. For future analysis purposes, it could have been benefitial to analyze text with a lot of words in capital letters, by adding a flag variable.\n",
    "\n",
    "2. remove the words counting just one letter\n",
    "\n",
    "Idem step one.\n",
    "\n",
    "3. remove the words that contain numbers\n",
    "\n",
    "Idem step one.\n",
    "\n",
    "4. tokenize the text and remove punctuation\n",
    "\n",
    "We performed tokenization with the base python .string function, to split sentences into words (tokens). \n",
    "\n",
    "5. remove all stop words\n",
    "\n",
    "Relevant analysis of the text depends on the most recurring words. Stopwords including words as \"the\", \"as\" and \"and\" appear a lot in a text, but do not give relevant explanation. For this reason they are removed.\n",
    "\n",
    "6. remove tokens that are empty\n",
    "\n",
    "After tokenization, we have to make sure all tokens taken into account contribute to the label prediction.\n",
    "\n",
    "7. pos tag the text\n",
    "\n",
    "We use the pos_tag function included in the ntlk library. This classifies our tokenized words as a noun, verb, adjective or adverb and adds to the understaning of the articles.\n",
    "\n",
    "8. lemmatize the text\n",
    "\n",
    "In order to normalize the text, we apply lemmatization. In this way, words with the same root are processed equally e.g. when took or taken are read in the text, they are lemmatized to take, infinitive of the two verbs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['preprocessed_text'] = df['title_and_text'].apply(lambda x: preprocess(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save preprocessed df\n",
    "df.to_csv(\"fake_or_real_news_train_PREPROCESSED.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"fake_or_real_news_train_PREPROCESSED.csv\")\n",
    "df = df.astype(object).replace(np.nan, 'None')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Train and Test again after pre-processing is done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder, train, test, train_cv, train_holdout, train_cv_label, train_holdout_label = split_train_holdout_test(encoder, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we create a dataframe called models to keep track of different models and their scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = pd.DataFrame(columns=['model_name', 'model_object', 'score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorizing dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For any text to be fed to a model, the text has to be transformed into numerical values. This process is called vectorizing and will be redone everytime a new feature is added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer(analyzer = \"word\")\n",
    "\n",
    "count_vectorizer = count_vect.fit(df.preprocessed_text)\n",
    "\n",
    "train_cv_vector = count_vectorizer.transform(train_cv.preprocessed_text)\n",
    "train_holdout_vector = count_vectorizer.transform(train_holdout.preprocessed_text)\n",
    "test_vector = count_vectorizer.transform(test.preprocessed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect.get_feature_names()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Model 1: SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a baseline classification model with a support vector machine, a good model to handle complex classifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVC_classifier = runModel(encoder,\n",
    "               train_cv_vector,\n",
    "               train_cv_label,\n",
    "               train_holdout_vector,\n",
    "               train_holdout.label,\n",
    "               \"svc\",\n",
    "               \"Baseline Model 1: SVC\")\n",
    "models.loc[len(models)] = SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Model 2: Naïve Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from IPython.display import Image\n",
    "Image(\"/Users/Gerald/Personal Drive/IE MBD/Term III/Natural Language Processing/Ass 1/NLP Fake News Predection | Gerald | Hatem/real_vs_fake.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this hand drawn example (text: *rude hell worth*), we explain why the Naïve Bayes model is helpful for our classification. The labels Real and Fake text are hidden, but every word, based on our training data, has a certain probability to belong to one of the two categories. The final score is calculated, multiplying all probabilities of the words (0.006 for real, 0.288 for fake). The algo thus does not take into account the order of the words in the multiplication. *rude hell worth* will be classified as fake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB = runModel(encoder,\n",
    "              train_cv_vector,\n",
    "              train_cv_label,\n",
    "              train_holdout_vector,\n",
    "              train_holdout.label,\n",
    "              \"nb\",\n",
    "              \"Baseline Model 2: Naiive Bayes\")\n",
    "models.loc[len(models)] = NB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Model 3: MaxEnt Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxEnt = runModel(encoder,\n",
    "              train_cv_vector,\n",
    "              train_cv_label,\n",
    "              train_holdout_vector,\n",
    "              train_holdout.label,\n",
    "              \"maxEnt\",\n",
    "              \"Baseline Model 3: MaxEnt Classifier\")\n",
    "models.loc[len(models)] = maxEnt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Models Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engin|eering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Explicit POS tagging\n",
    "- TF-IDF weighting\n",
    "- Bigram Count Vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "==> Select Final Model and predict on test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. POS Tagging\n",
    "\n",
    "Adding a prefix to each word with its type (Noun, Verb, Adjective,...).\n",
    "e.g: I went to school => PRP-I VBD-went TO-to NN-school\n",
    "\n",
    "Also, after lemmatization it will be 'VB-go NN-school', which indicates the semantics and distinguishes the purpose of the sentence.\n",
    "\n",
    "This will help the classifier differentiate between different types of sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['pos_tagged_text'] = df['preprocessed_text'].apply(lambda x: pos_tag_words(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rerun Models on pos-tagged text (FE1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder, train, test, train_cv, train_holdout, train_cv_label, train_holdout_label = split_train_holdout_test(encoder, df, False)\n",
    "\n",
    "count_vect = CountVectorizer(analyzer = \"word\")\n",
    "\n",
    "count_vectorizer = count_vect.fit(df.preprocessed_text)\n",
    "\n",
    "train_cv_vector = count_vectorizer.transform(train_cv.pos_tagged_text)\n",
    "train_holdout_vector = count_vectorizer.transform(train_holdout.pos_tagged_text)\n",
    "test_vector = count_vectorizer.transform(test.pos_tagged_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a. SVC with FE1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVC_pos_tag = runModel(encoder,\n",
    "               train_cv_vector,\n",
    "               train_cv_label,\n",
    "               train_holdout_vector,\n",
    "               train_holdout.label,\n",
    "               \"svc\",\n",
    "               \"SVC on pos-tagged text\")\n",
    "models.loc[len(models)] = SVC_pos_tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b. NB_pos_tag with FE1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_pos_tag = runModel(encoder,\n",
    "              train_cv_vector,\n",
    "              train_cv_label,\n",
    "              train_holdout_vector,\n",
    "              train_holdout.label,\n",
    "              \"nb\",\n",
    "              \"Naiive Bayes on pos-tagged text\")\n",
    "models.loc[len(models)] = NB_pos_tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c. maxEnt with FE1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxEnt_pos_tag = runModel(encoder,\n",
    "              train_cv_vector,\n",
    "              train_cv_label,\n",
    "              train_holdout_vector,\n",
    "              train_holdout.label,\n",
    "              \"maxEnt\",\n",
    "              \"MaxEnt Classifier on pos-tagged text\")\n",
    "models.loc[len(models)] = maxEnt_pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:blue\">\n",
    "There seems to be a slight increase in Accuracy after pos-tagging.\n",
    "</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. TF-IDF weighting\n",
    "\n",
    "Try to add weight to each word using TF-IDF\n",
    "<img src=\"https://cdn-images-1.medium.com/max/800/1*_OsV8gO2cjy9qcFhrtCdiw.jpeg\" width=\"350px\"/>\n",
    "\n",
    "We are going to calculate the TFIDF score of each term in a piece of text. The text will be tokenized into sentences and each sentence is then considered a text item.\n",
    "\n",
    "We will also apply those on the cleaned text and the concatinated POS_tagged text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"clean_and_pos_tagged_text\"] = df['preprocessed_text'] + ' ' + df['pos_tagged_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder, train, test, train_cv, train_holdout, train_cv_label, train_holdout_label = split_train_holdout_test(encoder, df, False)\n",
    "\n",
    "count_vect = CountVectorizer(analyzer = \"word\")\n",
    "\n",
    "count_vectorizer = count_vect.fit(df.clean_and_pos_tagged_text)\n",
    "\n",
    "train_cv_vector = count_vectorizer.transform(train_cv.clean_and_pos_tagged_text)\n",
    "train_holdout_vector = count_vectorizer.transform(train_holdout.clean_and_pos_tagged_text)\n",
    "test_vector = count_vectorizer.transform(test.clean_and_pos_tagged_text)\n",
    "\n",
    "\n",
    "tf_idf = TfidfTransformer(norm=\"l2\")\n",
    "train_cv_tf_idf = tf_idf.fit_transform(train_cv_vector)\n",
    "train_holdout_tf_idf = tf_idf.fit_transform(train_holdout_vector)\n",
    "test_tf_idf = tf_idf.fit_transform(test_vector)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rerun Models on preprocessed + pos-tagged (FE1) + TF-IDF weighted text (FE2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a. SVC with FE1 and FE2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVC_tf_idf = runModel(encoder,\n",
    "               train_cv_tf_idf,\n",
    "               train_cv_label,\n",
    "               train_holdout_tf_idf,\n",
    "               train_holdout.label,\n",
    "               \"svc\",\n",
    "               \"SVC on preprocessed+pos-tagged TF-IDF weighted text\")\n",
    "models.loc[len(models)] = SVC_tf_idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b. NB with FE1 and FE2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_tf_idf = runModel(encoder,\n",
    "               train_cv_tf_idf,\n",
    "               train_cv_label,\n",
    "               train_holdout_tf_idf,\n",
    "               train_holdout.label,\n",
    "              \"nb\",\n",
    "              \"Naiive Bayes on preprocessed+pos-tagged TF-IDF weighted text\")\n",
    "models.loc[len(models)] = NB_tf_idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c. maxEnt with FE1 and FE2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxEnt_tf_idf = runModel(encoder,\n",
    "               train_cv_tf_idf,\n",
    "               train_cv_label,\n",
    "               train_holdout_tf_idf,\n",
    "               train_holdout.label,\n",
    "              \"maxEnt\",\n",
    "              \"MaxEnt on preprocessed+pos-tagged TF-IDF weighted text\")\n",
    "models.loc[len(models)] = maxEnt_tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:blue\">\n",
    "Using TF-IDF increased the score to ~94.5% with SVC and Max-Ent models.\n",
    "<br><br>\n",
    "Naive-Bayes rather decreased the score. Therefore we drop it from the pipeline.\n",
    "</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Use Bigram Vectorizer instead of regular vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For FE3, we use the Trigram vectorizer, which vectorizes **triplets of words** rather than each word separately. *In this short example sentence*, the trigrams are \"In this short\", \"this short example\" and \"short example sentence\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder, train, test, train_cv, train_holdout, train_cv_label, train_holdout_label = split_train_holdout_test(encoder, df, False)\n",
    "\n",
    "trigram_vect = CountVectorizer(analyzer = \"word\", ngram_range=(1,2))\n",
    "\n",
    "trigram_vect = count_vect.fit(df.clean_and_pos_tagged_text)\n",
    "\n",
    "train_cv_vector = trigram_vect.transform(train_cv.clean_and_pos_tagged_text)\n",
    "train_holdout_vector = trigram_vect.transform(train_holdout.clean_and_pos_tagged_text)\n",
    "test_vector = trigram_vect.transform(test.clean_and_pos_tagged_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf = TfidfTransformer(norm=\"l2\")\n",
    "train_cv_bigram_tf_idf = tf_idf.fit_transform(train_cv_vector)\n",
    "train_holdout_bigram_tf_idf = tf_idf.fit_transform(train_holdout_vector)\n",
    "test_bigram_tf_idf = tf_idf.fit_transform(test_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rerun Models on preprocessed + pos-tagged (FE1) + TF-IDF weighted (FE2) + Trigram vectorized text (FE3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a. SVC with FE1, FE2 and FE3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVC_trigram_tf_idf = runModel(encoder,\n",
    "               train_cv_bigram_tf_idf,\n",
    "               train_cv_label,\n",
    "               train_holdout_bigram_tf_idf,\n",
    "               train_holdout.label,\n",
    "               \"svc\",\n",
    "               \"SVC on bigram vect.+ TF-IDF\")\n",
    "models.loc[len(models)] = SVC_trigram_tf_idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b. maxEnt with FE1, FE2 and FE3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder, train, test, train_cv, train_holdout, train_cv_label, train_holdout_label = split_train_holdout_test(encoder, df, False)\n",
    "\n",
    "trigram_vect = CountVectorizer(analyzer = \"word\", ngram_range=(1,3))\n",
    "\n",
    "trigram_vect = count_vect.fit(df.clean_and_pos_tagged_text)\n",
    "\n",
    "train_cv_vector = trigram_vect.transform(train_cv.clean_and_pos_tagged_text)\n",
    "train_holdout_vector = trigram_vect.transform(train_holdout.clean_and_pos_tagged_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf = TfidfTransformer(norm=\"l2\")\n",
    "train_cv_trigram_tf_idf = tf_idf.fit_transform(train_cv_vector)\n",
    "train_holdout_trigram_tf_idf = tf_idf.fit_transform(train_holdout_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxEnt_tf_idf = runModel(encoder,\n",
    "               train_cv_trigram_tf_idf,\n",
    "               train_cv_label,\n",
    "               train_holdout_trigram_tf_idf,\n",
    "               train_holdout.label,\n",
    "              \"maxEnt\",\n",
    "              \"MaxEnt on trigram vect.+ TF-IDF\")\n",
    "models.loc[len(models)] = maxEnt_tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:blue\">\n",
    "It looks like the \"MaxEnt on trigram vect.+ TF-IDF\" is the best model with the highest score. We will use it to predict and classify the testset.\n",
    "</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting on test dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Train on whole data and predict on test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PREPROCESSED data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"fake_or_real_news_test.csv\")\n",
    "train = pd.read_csv(\"fake_or_real_news_training_CLEANED.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['title_and_text'] = train['title'] +' '+ train['text']\n",
    "train['preprocessed_text'] = train['title_and_text'].apply(lambda x: preprocess(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['title_and_text'] = test['title'] +' '+ test['text']\n",
    "test['preprocessed_text'] = test['title_and_text'].apply(lambda x: preprocess(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save preprocessed df\n",
    "train.to_csv(\"fake_or_real_news_train_PREPROCESSED.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save preprocessed df\n",
    "test.to_csv(\"fake_or_real_news_test_PREPROCESSED.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"fake_or_real_news_train_PREPROCESSED.csv\")\n",
    "train = train.astype(object).replace(np.nan, 'None')\n",
    "\n",
    "test = pd.read_csv(\"fake_or_real_news_test_PREPROCESSED.csv\")\n",
    "test = test.astype(object).replace(np.nan, 'None')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test.astype(object).replace(np.nan, 'None')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['pos_tagged_text'] = train['preprocessed_text'].apply(lambda x: pos_tag_words(x))\n",
    "test['pos_tagged_text'] = test['preprocessed_text'].apply(lambda x: pos_tag_words(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### merge clean and pos tagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"clean_and_pos_tagged_text\"] = train['preprocessed_text'] + ' ' + train['pos_tagged_text']\n",
    "test[\"clean_and_pos_tagged_text\"] = test['preprocessed_text'] + ' ' + train['pos_tagged_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling using MaxEnt on trigram vect.+ TF-IDF Grid Search Best params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trigram + Tfdif + classifier pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "trigram_vectorizer = CountVectorizer(analyzer = \"word\", ngram_range=(1,3))\n",
    "tf_idf = TfidfTransformer(norm=\"l2\")\n",
    "classifier = LogisticRegression(C=1000, class_weight=None, dual=False, fit_intercept=True,\n",
    "          intercept_scaling=1, max_iter=100, multi_class='multinomial',\n",
    "          n_jobs=None, penalty='l2', random_state=None, solver='saga',\n",
    "          tol=0.0001, verbose=0, warm_start=False)\n",
    "\n",
    "pipeline = Pipeline([\n",
    "     ('trigram_vectorizer', trigram_vectorizer),\n",
    "     ('tfidf', tf_idf),\n",
    "     ('clf', classifier),\n",
    " ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.fit(train.clean_and_pos_tagged_text, encoder.fit_transform(train.label.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump( pipeline, open( \"pipeline.pkl\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Predicting on test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(colored(\"Predicting on test\", 'blue'))\n",
    "test_predictions = test_predictions = pipeline.predict(test.clean_and_pos_tagged_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions_decoded = encoder.inverse_transform( test_predictions )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = test\n",
    "predictions[\"label\"] = test_predictions_decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.label.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "ax = sns.countplot(predictions.label,\n",
    "                order=[x for x, count in sorted(collections.Counter(predictions.label).items(),\n",
    "                key=lambda x: -x[1])])\n",
    "\n",
    "\n",
    "for p in ax.patches:\n",
    "    height = p.get_height()\n",
    "    ax.text(p.get_x()+p.get_width()/2.,\n",
    "            height + 3,\n",
    "            '{:1.2f}%'.format(height/len(predictions)*100),\n",
    "            ha=\"center\") \n",
    "ax.set_title(\"Test dataset target\")\n",
    "show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.drop(columns=[\"title\",\"text\",\"title_and_text\",\"preprocessed_text\",\"pos_tagged_text\",\"clean_and_pos_tagged_text\"]).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.to_csv(\"TEST_PREDICTIONS.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
